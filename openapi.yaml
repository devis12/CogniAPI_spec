swagger: '2.0'
schemes:
  - http
  - https
host: cogni-api.herokuapp.com
basePath: /
info:
  description: |
    CogniAPI aims to provide you a simpler and more straightforward way to analyse and process images content expoiting the power of Azure and Google Cloud cognitive services
    # Introduction
    This specification is intended to to be a good starting point for describing CogniAPI in 
    [OpenAPI/Swagger format](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md).

    # CogniAPI Specification
    The goal of The CogniAPI Specification is to guide you to the use of a less intricate but very useful REST service for image analysis which could be applied in many cases. Exploiting the power of Azure Computer Vision, Azure Face and Google Cloud Vision, this APIs will try to provide all the results to you in a unified and more straightforward schema. The properties provided to you could be resume in the following list:

      * Retrieving tags which tries to condensate the image content in a set of words
      * Detecting objects, locations and people of interest
      * Detecting faces (with emotions, gender, age and additional attributes related to them)
      * Evaluating the presence of inappropriate content in the image (racy, adult, violence, ...)
      * Complete analysis of the colors and other graphical aspects which characterize the image
  version: 1.0.0
  title: CogniAPI
  termsOfService: 'https://cogni-api.herokuapp.com/terms/'
  contact:
    email: devis.dalmoro@studenti.unitn.it
  license:
    name: MIT
    url: 'https://github.com/devis12/CogniAPI/blob/master/LICENSE'
  x-logo:
    url: 'https://github.com/devis12/CogniAPI/blob/master/views/img/logo.png'
externalDocs:
  description: Find out how to create Github repo for your OpenAPI spec.
  url: 'https://github.com/Rebilly/generator-openapi-repo'
produces:
  - application/json
  - text/plain
consumes:
  - application/json
  - application/xml
tags:
  - name: Analyse
    description: Image Analysis operations provided by CogniAPI
  - name: AsyncAnalyse
    description: Image Async Analysis operations provided by CogniAPI
  - name: GCloud
    description: Image Analysis operations provided just by Google Cloud Vision
  - name: Azure
    description: Image Analysis operations provided just by Azure Computer Vision and Azure Face
  - name: AzureFaceMg
    description: Face training operations provided just by Azure Face
paths:
  /analyse:
    get:
      tags:
        - Analyse
      summary: Complete image analysis
      description: |
        This call will probably be the most useful since it combines in a 
        single response all the valuable information extracted from the cognitive services mentioned above and showing them in a unified schema
      operationId: analyse
      parameters:
        - in: query
          name: user
          type: string
          required: false
          description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
        - in: query
          name: minscore
          type: number
          required: false
          default: 0
          minimum: 0
          maximum: 1
          description: 'Optional score value. You can set the minimum threshold in order to filter out tags and objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)'
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/ImageAnnotation'
        '400':
          description: Bad Request
  /analyse/faces:
    get:
      tags:
        - Analyse
      summary: Analysis of detected faces in the image
      description: |
        This call will provide a full face detection and recogntion of the image (the second one will be performed just if the user is logged)
      operationId: analyseFace
      parameters:
        - in: query
          name: user
          type: string
          required: false
          description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Face'
        '400':
          description: Bad Request
  /analyse/tags:
    get:
      tags:
        - Analyse
      summary: Image tag analysis
      description: |
        This call provides just tag and labels, respect to what it's possible to obtain with a full and complete /analyse call
      operationId: analyseTag
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
        - in: query
          name: minscore
          type: number
          required: false
          default: 0
          minimum: 0
          maximum: 1
          description: 'Optional score value. You can set the minimum threshold in order to filter out tags with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)'
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
  /analyse/objects:
    get:
      tags:
        - Analyse
      summary: Image objects detection
      description: |
        This call provides just detected objects, respect to what it's possible to obtain with a full and complete /analyse call
      operationId: analyseObject
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
        - in: query
          name: minscore
          type: number
          required: false
          default: 0
          minimum: 0
          maximum: 1
          description: 'Optional score value. You can set the minimum threshold in order to filter out objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)'
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Tag'
        '400':
          description: Bad Request
  /analyse/description:
    get:
      tags:
        - Analyse
      summary: Description of image content
      description: |
        This call provides a brief description of the image content: there will be a bunch of generic tags, grouped by concept, some captions and a classification based on a 86 category taxonomy defined here https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/category-taxonomy
      operationId: analyseDescription
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/Description'
        '400':
          description: Bad Request
  /analyse/texts:
    get:
      tags:
        - Analyse
      summary: Text detection of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: analyseText
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Text'
        '400':
          description: Bad Request
  /analyse/landmarks:
    get:
      tags:
        - Analyse
      summary: Landmark detection of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: analyseLandmarks
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Landmark'
        '400':
          description: Bad Request
  /analyse/safety:
    get:
      tags:
        - Analyse
      summary: Safety tags annotation of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: analyseSafety
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/SafetyAnnotation'
        '400':
          description: Bad Request
  /analyse/colors:
    get:
      tags:
        - Analyse
      summary: Color analysis of image content
      description: |
        This call provides a complete analysis of the image colors 
      operationId: analyseColor
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/GraphicalData'
        '400':
          description: Bad Request
  /gcloud/analyse:
    get:
      tags:
        - GCloud
      summary: Complete image analysis
      description: |
        This call will provide in a single response all the valuable information extracted from the cognitive services using just gcloud vision and showing them in a unified schema (no need to authenticate yourself because google cloud vision doesn't supply any face recognition service)
      operationId: GCloudAnalyse
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/GCloudImageAnnotation'
        '400':
          description: Bad Request
  /gcloud/faces:
    get:
      tags:
        - GCloud
      summary: Face detected in the image
      description: |
        This call will provide a full face detection of the image with the data which google cloud provide
      operationId: GCloudAnalyseFace
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/GCloudFace'
        '400':
          description: Bad Request
  /gcloud/tags:
    get:
      tags:
        - GCloud
      summary: Image tag analysis
      description: |
        This call provides just tag and labels, respect to what it's possible to obtain with a full and complete /analyse call
      operationId: GCloudAnalyseTag
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
  /gcloud/objects:
    get:
      tags:
        - GCloud
      summary: Image objects detection
      description: |
        This call provides just detected objects, respect to what it's possible to obtain with a full and complete /analyse call
      operationId: GCloudAnalyseObject
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Tag'
        '400':
          description: Bad Request
  /gcloud/description:
    get:
      tags:
        - GCloud
      summary: Description of image content
      description: |
        This call provides a brief description of the image content: there will be a bunch of generic tags, grouped by concept
      operationId: GCloudAnalyseDescription
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/GCloudDescription'
        '400':
          description: Bad Request
  /gcloud/texts:
    get:
      tags:
        - GCloud
      summary: Text detection of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: GCloudAnalyseText
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Text'
        '400':
          description: Bad Request
  /gcloud/landmarks:
    get:
      tags:
        - GCloud
      summary: Landmark detection of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: GCloudAnalyseLandmarks
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Landmark'
        '400':
          description: Bad Request
  /gcloud/safety:
    get:
      tags:
        - GCloud
      summary: Safety tags annotation of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: GCloudAnalyseSafety
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/GCloudSafetyAnnotation'
        '400':
          description: Bad Request
  /gcloud/colors:
    get:
      tags:
        - GCloud
      summary: Color analysis of image content
      description: |
        This call provides a complete analysis of the image colors 
      operationId: GCloudAnalyseColor
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/GCloudGraphicalData'
        '400':
          description: Bad Request
  /azure/analyse:
    get:
      tags:
        - Azure
      summary: Complete image analysis
      description: |
        This call will combine in a single response all the valuable information extracted from Azure Computer Vision and Azure Face apis showing them in a unified schema (in respect to the standard one)
      operationId: AzureAnalyse
      parameters:
        - in: query
          name: user
          type: string
          required: false
          description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/AzureImageAnnotation'
        '400':
          description: Bad Request
  /azure/faces:
    get:
      tags:
        - Azure
      summary: Face detected in the image with recognition
      description: |
        This call will provide a full face detection of the image with the data which google cloud provide
      operationId: AzureAnalyseFace
      parameters:
        - in: query
          name: user
          type: string
          required: false
          description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/AzureFace'
        '400':
          description: Bad Request
  /azure/tags:
    get:
      tags:
        - Azure
      summary: Image tag analysis
      description: |
        This call provides just tag and labels, respect to what it's possible to obtain with a full and complete /analyse call
      operationId: AzureAnalyseTag
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
  /azure/objects:
    get:
      tags:
        - Azure
      summary: Image objects detection
      description: |
        This call provides just detected objects, respect to what it's possible to obtain with a full and complete /analyse call
      operationId: AzureAnalyseObject
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/Tag'
        '400':
          description: Bad Request
  /azure/description:
    get:
      tags:
        - Azure
      summary: Description of image content
      description: |
        This call provides a brief description of the image content: there will be a bunch of generic tags, grouped by concept
      operationId: AzureAnalyseDescription
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/Description'
        '400':
          description: Bad Request
  /azure/landmarks:
    get:
      tags:
        - Azure
      summary: Landmark detection of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: AzureAnalyseLandmarks
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            type: array
            items:
              $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
  /azure/safety:
    get:
      tags:
        - Azure
      summary: Safety tags annotation of supplied image
      description: |
        This call will return the text which has been detected inside the image
      operationId: AzureAnalyseSafety
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/AzureSafetyAnnotation'
        '400':
          description: Bad Request
  /azure/colors:
    get:
      tags:
        - Azure
      summary: Color analysis of image content
      description: |
        This call provides a complete analysis of the image colors 
      operationId: AzureAnalyseColor
      parameters:
        - in: query
          name: url
          type: string
          required: true
          description: Link of the image which has to be analysed
      produces:
        - application/json
      responses:
        '200':
          description: Success
          schema:
            $ref: '#/definitions/AzureGraphicalData'
        '400':
          description: Bad Request
  '/azure/faces/{user}':
    post:
      tags:
        - AzureMg
      summary: Add face to your face group in order to persist some user data related to it and recognize it when in future image submissions there will be a similar one
      description: |
        This call provides a way for you to store some user data related to a face, so they will pop out again when a face similar to the one you're storing will be detected (mandatory to perform face training in the between order to achieve this final result)
      operationId: AzureAddFace
      consumes:
        - application/json
        - application/x-www-form-urlencoded
      parameters:
        - in: path
          name: user
          type: string
          required: true
          description: 'User token which can be used to identify the logged user, so to perform operations on the face group related to him'
        - in: formData
          name: imageUrl
          type: string
          required: true
          description: Url of the image in which the face has been detected
        - in: formData
          name: target
          type: string
          required: true
          description: 'left,top,width,height values passed in this very order as a string separated by comma values (example ''232,435,190,195''). You can retrieve this value from precedent analysis and face detection, where the top value it''s just the y of the {tl} or {tr} corner and the left value it''s the x coordinate of the {bl} or {tl} corner (width and height are already given in the faceRectangle)'
        - in: formData
          name: userData
          type: string
          required: true
          description: User data you want to associate to this face
      produces:
        - application/json
      responses:
        '200':
          description: Success
        '400':
          description: Bad Request
    patch:
      tags:
        - AzureMg
      summary: Update face to your face group in order to persist new user data related to it and recognize it when in future image submissions there will be a similar one
      description: |
        This call provides a way for you to update the stored user data related to a face, so they will pop out again when a face similar to the one you're storing will be detected (mandatory to perform face training in the between order to achieve this final result)
      operationId: AzurePatchFace
      consumes:
        - application/json
        - application/x-www-form-urlencoded
      parameters:
        - in: path
          name: user
          type: string
          required: true
          description: 'User token which can be used to identify the logged user, so to perform operations on the face group related to him'
        - in: formData
          name: persistedFaceId
          type: string
          required: true
          description: Id of the persisted face (you can have it in the similarFace arrat resulted from previous analysis)
        - in: formData
          name: userData
          type: string
          required: true
          description: User data you want to associate to this face
      produces:
        - application/json
      responses:
        '200':
          description: Success
        '400':
          description: Bad Request
    delete:
      tags:
        - AzureMg
      summary: Update face to your face group in order to persist new user data related to it and recognize it when in future image submissions there will be a similar one
      description: |
        This call provides a way for you to delete the stored user data related to a face, so they won't pop out again when a face similar to the one you're storing will be detected (mandatory to perform face training in the between order to achieve this final result)
      operationId: AzureDeleteFace
      consumes:
        - application/json
        - application/x-www-form-urlencoded
      parameters:
        - in: path
          name: user
          type: string
          required: true
          description: 'User token which can be used to identify the logged user, so to perform operations on the face group related to him'
        - in: formData
          name: persistedFaceId
          type: string
          required: true
          description: Id of the persisted face (you can have it in the similarFace arrat resulted from previous analysis)
      produces:
        - application/json
      responses:
        '200':
          description: Success
        '400':
          description: Bad Request
  '/azure/faces/train/{user}':
    post:
      tags:
        - AzureMg
      summary: 'Train your face group (mandatory after adding, patching or deleting face operations)'
      description: |
        This call provides a way for you to start training your cognitive service, in order to have a good face recognition system 
      operationId: AzureTrainFace
      parameters:
        - in: path
          name: user
          type: string
          required: true
          description: 'User token which can be used to identify the logged user, so to perform operations on the face group related to him'
      produces:
        - application/json
      responses:
        '200':
          description: Success
        '400':
          description: Bad Request
definitions:
  Point2d:
    type: object
    properties:
      x:
        description: X coordinate in px
        type: integer
      'y':
        description: Y coordinate in px
        type: integer
  Point3d:
    type: object
    properties:
      x:
        description: X coordinate in px
        type: integer
      'y':
        description: Y coordinate in px
        type: integer
      z:
        description: Z coordinate in px (depth in an image)
        type: integer
  BoundingBox:
    type: object
    properties:
      width:
        description: Width dimension in px of the rectangle box
        type: integer
      height:
        description: Height dimension in px of the rectangle box
        type: integer
      bl:
        $ref: '#/definitions/Point2d'
      br:
        $ref: '#/definitions/Point2d'
      tl:
        $ref: '#/definitions/Point2d'
      tr:
        $ref: '#/definitions/Point2d'
  Exposure:
    type: object
    properties:
      exposureLevel:
        description: Face exposure level
        type: string
        enum:
          - GoodExposure
          - OverExposure
          - UnderExposure
      value:
        description: Level of exposure expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
      underExposedLikelihood:
        description: Google Cloud Likelihood value in order to express the level of exposure
        type: string
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - 'UNLIKELY, POSSIBLE'
          - LIKELY
          - VERY_LIKELY
  AzureExposure:
    type: object
    properties:
      exposureLevel:
        description: Face exposure level
        type: string
        enum:
          - GoodExposure
          - OverExposure
          - UnderExposure
      value:
        description: Level of exposure expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
  Blur:
    type: object
    properties:
      blurLevel:
        description: Blur level
        type: string
        enum:
          - Low
          - Medium
          - High
      value:
        description: Level of blur expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
      blurredLikelihood:
        description: Google Cloud Likelihood value in order to express the level of blurriness
        type: string
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - 'UNLIKELY, POSSIBLE'
          - LIKELY
          - VERY_LIKELY
  AzureBlur:
    type: object
    properties:
      blurLevel:
        description: Blur level
        type: string
        enum:
          - Low
          - Medium
          - High
      value:
        description: Level of blur expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
  Noise:
    type: object
    properties:
      noiseLevel:
        description: Noise level of face pixels
        type: string
        enum:
          - Low
          - Medium
          - High
      value:
        description: Level of noise of face pixels expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
  Occlusion:
    type: object
    properties:
      eyeMakeup:
        description: True if the eye(s) is(are) detected with makeup on it(them)
        type: boolean
      lipMakeup:
        description: True if the lip(s) has(have) makeup on it(them)
        type: boolean
  Makeup:
    type: object
    properties:
      foreheadOccluded:
        description: True if the forehead is occluded
        type: boolean
      eyeOccluded:
        description: True if the eye(s) is(are) occluded
        type: boolean
      mouthOccluded:
        description: True if the mouth is occluded
        type: boolean
  Accessory:
    type: object
    properties:
      type:
        description: Denomination of the type of identified accessory
        type: string
        example: headwear
      confidence:
        description: Probability value between 0 and 1 in order to express the confidence of the presence of the identified object
        type: number
        minimum: 0
        maximum: 1
  Emotion:
    type: object
    properties:
      confidence:
        description: Probability value between 0 and 1 in order to express the confidence of the presence of the identified object
        type: number
        minimum: 0
        maximum: 1
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
  AzureEmotion:
    type: object
    properties:
      confidence:
        description: Probability value between 0 and 1 in order to express the confidence of the presence of the identified object
        type: number
        minimum: 0
        maximum: 1
  GCloudEmotion:
    type: object
    properties:
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
  HeadPose:
    type: object
    properties:
      rollAngle:
        description: 'Roll angle, which indicates the amount of clockwise/anti-clockwise rotation of the face relative to the image vertical about the axis perpendicular to the face. Range [-180,180]'
        type: number
        minimum: -180
        maximum: 180
      panAngle:
        description: 'Yaw angle, which indicates the leftward/rightward angle that the face is pointing relative to the vertical plane perpendicular to the image. Range [-180,180].'
        type: number
        minimum: -180
        maximum: 180
      tiltAngle:
        description: 'Pitch angle, which indicates the upwards/downwards angle that the face is pointing relative to the image''s horizontal plane. Range [-180,180].'
        type: number
        minimum: -180
        maximum: 180
  Hair:
    type: object
    properties:
      invisible:
        description: Indicate if the hair are visible or not
        type: boolean
      bald:
        description: Indicate level of baldness as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
      hairColors:
        description: Possible colors of the detected hair for the face
        type: array
        items:
          $ref: '#/definitions/HairColor'
  HairColor:
    type: object
    properties:
      color:
        description: Possible color for the hair
        type: string
        example: Brown
      confidence:
        description: Confidence value for this hair color expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
  FacialHair:
    type: object
    properties:
      moustache:
        description: Indicate if it's been detected a moustache in the face
        type: boolean
      beard:
        description: Indicate if it's been detected a beard in the face
        type: boolean
      sideburns:
        description: Indicate if it's been detected a sideburns in the face
        type: boolean
  PersistedFace:
    type: object
    properties:
      persistedFaceId:
        description: Persisted Face Id on Azure in the Face List Group related to the logged user
        type: string
      userData:
        description: User Data linked to the persisted Face Id on Azure in the Face List Group related to the logged user (and provided by him/her in the first place)
        type: string
      confidence:
        description: Level of similarity of this persisted face in respect to the one which has been just detected
        type: number
        minimum: 0
        maximum: 1
  FaceLandmarksAnnotation:
    type: object
    properties:
      type:
        type: string
        enum:
          - UNKNOWN_LANDMARK
          - LEFT_EYE
          - RIGHT_EYE
          - LEFT_OF_LEFT_EYEBROW
          - RIGHT_OF_LEFT_EYEBROW
          - LEFT_OF_RIGHT_EYEBROW
          - RIGHT_OF_RIGHT_EYEBROW
          - MIDPOINT_BETWEEN_EYES
          - NOSE_TIP
          - UPPER_LIP
          - LOWER_LIP
          - MOUTH_LEFT
          - MOUTH_RIGHT
          - MOUTH_CENTER
          - NOSE_BOTTOM_RIGHT
          - NOSE_BOTTOM_LEFT
          - NOSE_BOTTOM_CENTER
          - LEFT_EYE_TOP_BOUNDARY
          - LEFT_EYE_RIGHT_CORNER
          - LEFT_EYE_BOTTOM_BOUNDARY
          - LEFT_EYE_LEFT_CORNER
          - RIGHT_EYE_TOP_BOUNDARY
          - RIGHT_EYE_RIGHT_CORNER
          - RIGHT_EYE_BOTTOM_BOUNDARY
          - RIGHT_EYE_LEFT_CORNER
          - LEFT_EYEBROW_UPPER_MIDPOINT
          - RIGHT_EYEBROW_UPPER_MIDPOINT
          - LEFT_EAR_TRAGION
          - RIGHT_EAR_TRAGION
          - LEFT_EYE_PUPIL
          - RIGHT_EYE_PUPIL
          - FOREHEAD_GLABELLA
          - CHIN_GNATHION
          - CHIN_LEFT_GONION
          - CHIN_RIGHT_GONION
      position:
        $ref: '#/definitions/Point3d'
  TagNoBound:
    type: object
    properties:
      name:
        description: 'Concept, object, logo, celebrity name'
        type: string
      confidence:
        description: 'Level of confidence for the detected concept, logo, object, celebrity'
        type: number
        minimum: 0
        maximum: 1
  Tag:
    type: object
    properties:
      name:
        description: 'Concept, object, logo, celebrity name'
        type: string
      confidence:
        description: 'Level of confidence for the detected concept, logo, object, celebrity'
        type: number
        minimum: 0
        maximum: 1
      boundingBox:
        $ref: '#/definitions/BoundingBox'
  Text:
    type: object
    properties:
      content:
        description: Content for the detected text
        type: string
      confidence:
        description: Level of confidence in relation to the presence of the detected text
        type: number
        minimum: 0
        maximum: 1
      boundingBox:
        $ref: '#/definitions/BoundingBox'
  Landmark:
    type: object
    properties:
      name:
        description: Name of the detected landmark
        type: string
      confidence:
        description: Level of confidence in relation to the presence of the detected landmark
        type: number
        minimum: 0
        maximum: 1
      latitude:
        description: Latitude coordinate for the detected landmark
        type: number
        minimum: -90
        maximum: 90
      longitude:
        description: Longitude coordinate for the detected landmark
        type: number
        minimum: -180
        maximum: 180
      boundingBox:
        $ref: '#/definitions/BoundingBox'
  Description:
    type: object
    properties:
      generic_tags:
        description: Array of generic tags related to the uploaded image (grouped in lists by common concepts)
        type: array
        items:
          type: array
          items:
            type: string
      captions:
        description: Brief sentences which aims to synthesize the content of the image
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      categories:
        description: 'Most of the time just a single word which aims to synthesize the content of the image. All the values are limited to an 86 taxonomy defined by Azure Computer Vision at https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/category-taxonomy'
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
  GCloudDescription:
    type: object
    properties:
      generic_tags:
        description: Array of generic tags related to the uploaded image (grouped in lists by common concepts)
        type: array
        items:
          type: array
          items:
            type: string
  RGBA:
    type: object
    properties:
      r:
        description: Red value for rgba
        type: integer
        minimum: 0
        maximum: 255
      g:
        description: Green value for rgba
        type: integer
        minimum: 0
        maximum: 255
      b:
        description: Blue value for rgba
        type: integer
        minimum: 0
        maximum: 255
      a:
        description: Alpha value
        type: number
        minimum: 0
        maximum: 1
  ColorInfoRGBA:
    type: array
    items:
      type: object
      properties:
        pixelFraction:
          description: 'The fraction of pixels the color occupies in the image. Value in range [0, 1]'
          type: number
          minimum: 0
          maximum: 1
        confidence:
          description: 'Image-specific score for this color. Value in range [0, 1].'
          type: number
          minimum: 0
          maximum: 1
        color:
          $ref: '#/definitions/RGBA'
  GraphicalData:
    type: object
    properties:
      dominantColorForeground:
        description: The dominant color in the foreground
        type: string
      dominantColorBackground:
        description: The dominant color in the background
        type: string
      dominantColors:
        description: The dominant colors for the image
        type: array
        items:
          type: string
      accentColor:
        description: Accent color detected for the current image
        type: string
      isBWImg:
        description: True if the image is in black and white
        type: boolean
      clipArtType:
        description: Integer value between 0 and 3 in order to define how much this image is similar to a clipart
        type: integer
        minimum: 0
        maximum: 3
      lineDrawingType:
        description: True if the image content is detected as drawn by a human
        type: boolean
      colorInfoRGBA:
        $ref: '#/definitions/ColorInfoRGBA'
  AzureGraphicalData:
    type: object
    properties:
      dominantColorForeground:
        description: The dominant color in the foreground
        type: string
      dominantColorBackground:
        description: The dominant color in the background
        type: string
      dominantColors:
        description: The dominant colors for the image
        type: array
        items:
          type: string
      accentColor:
        description: Accent color detected for the current image
        type: string
      isBWImg:
        description: True if the image is in black and white
        type: boolean
      clipArtType:
        description: Integer value between 0 and 3 in order to define how much this image is similar to a clipart
        type: integer
        minimum: 0
        maximum: 3
      lineDrawingType:
        description: True if the image content is detected as drawn by a human
        type: boolean
  GCloudGraphicalData:
    type: object
    properties:
      colorInfoRGBA:
        $ref: '#/definitions/ColorInfoRGBA'
  ConfidenceLabel:
    type: string
    description: Confidence value expressed as the likelihood label offered by Google Cloud Vision
    enum:
      - UNKNOWN
      - VERY_UNLIKELY
      - 'UNLIKELY, POSSIBLE'
      - LIKELY
      - VERY_LIKELY
  SafetyProperty:
    type: object
    properties:
      present:
        description: 'True if the content of the image confirm the presence of this property (property ''racy'' and present=True, it means that the content has a highly probability of been racist)'
        type: boolean
      confidence:
        description: Confidence value expressed as a value between 0 and 1 for this safety property
        type: number
        minimum: 0
        maximum: 1
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
  AzureSafetyProperty:
    type: object
    properties:
      present:
        description: 'True if the content of the image confirm the presence of this property (property ''racy'' and present=True, it means that the content has a highly probability of been racist)'
        type: boolean
      confidence:
        description: Confidence value expressed as a value between 0 and 1 for this safety property
        type: number
        minimum: 0
        maximum: 1
  GCloudSafetyProperty:
    type: object
    properties:
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
  SafetyAnnotation:
    type: object
    properties:
      racy:
        $ref: '#/definitions/SafetyProperty'
      adult:
        $ref: '#/definitions/SafetyProperty'
      violence:
        $ref: '#/definitions/SafetyProperty'
      medical:
        $ref: '#/definitions/SafetyProperty'
      spoof:
        $ref: '#/definitions/SafetyProperty'
  AzureSafetyAnnotation:
    type: object
    properties:
      racy:
        $ref: '#/definitions/AzureSafetyProperty'
      adult:
        $ref: '#/definitions/AzureSafetyProperty'
  GCloudSafetyAnnotation:
    type: object
    properties:
      racy:
        $ref: '#/definitions/GCloudSafetyProperty'
      adult:
        $ref: '#/definitions/GCloudSafetyProperty'
      violence:
        $ref: '#/definitions/GCloudSafetyProperty'
      medical:
        $ref: '#/definitions/GCloudSafetyProperty'
      spoof:
        $ref: '#/definitions/GCloudSafetyProperty'
  Metadata:
    type: object
    properties:
      width:
        description: Pixel width for the analyzed picture
        type: integer
      height:
        description: Pixel height for the analyzed picture
        type: integer
      format:
        description: Format of the analyzed picture
        type: string
  Face:
    type: object
    properties:
      faceId:
        description: faceId randomly generated by Azure Face (will expire in 24 hours)
        type: string
      gender:
        description: Recognized gender
        type: string
        enum:
          - male
          - female
      age:
        description: Recognized age value
        type: integer
      smile:
        description: 'Smile intensity, a number between [0,1]'
        type: number
        minimum: 0
        maximum: 1
      glasses:
        description: Glasses type
        type: string
        enum:
          - NoGlasses
          - ReadingGlasses
          - Sunglasses
          - SwimmingGoggles
      faceRectangle:
        $ref: '#/definitions/BoundingBox'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/FaceLandmarksAnnotation'
      similarFaces:
        type: array
        items:
          $ref: '#/definitions/PersistedFace'
      celebrity:
        $ref: '#/definitions/TagNoBound'
      facialHair:
        $ref: '#/definitions/FacialHair'
      hair:
        $ref: '#/definitions/Hair'
      headPose:
        $ref: '#/definitions/HeadPose'
      emotions:
        type: object
        properties:
          anger:
            $ref: '#/definitions/Emotion'
          contempt:
            $ref: '#/definitions/Emotion'
          disgust:
            $ref: '#/definitions/Emotion'
          fear:
            $ref: '#/definitions/Emotion'
          happiness:
            $ref: '#/definitions/Emotion'
          neutral:
            $ref: '#/definitions/Emotion'
          sadness:
            $ref: '#/definitions/Emotion'
          surprise:
            $ref: '#/definitions/Emotion'
      makeup:
        $ref: '#/definitions/Makeup'
      occlusion:
        $ref: '#/definitions/Occlusion'
      accessories:
        type: array
        items:
          $ref: '#/definitions/Accessory'
      noise:
        $ref: '#/definitions/Noise'
      blur:
        $ref: '#/definitions/Blur'
      exposure:
        $ref: '#/definitions/Exposure'
  AzureFace:
    type: object
    properties:
      faceId:
        description: faceId randomly generated by Azure Face (will expire in 24 hours)
        type: string
      gender:
        description: Recognized gender
        type: string
        enum:
          - male
          - female
      age:
        description: Recognized age value
        type: integer
      smile:
        description: 'Smile intensity, a number between [0,1]'
        type: number
        minimum: 0
        maximum: 1
      glasses:
        description: Glasses type
        type: string
        enum:
          - NoGlasses
          - ReadingGlasses
          - Sunglasses
          - SwimmingGoggles
      faceRectangle:
        $ref: '#/definitions/BoundingBox'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/FaceLandmarksAnnotation'
      similarFaces:
        type: array
        items:
          $ref: '#/definitions/PersistedFace'
      celebrity:
        $ref: '#/definitions/TagNoBound'
      facialHair:
        $ref: '#/definitions/FacialHair'
      hair:
        $ref: '#/definitions/Hair'
      headPose:
        $ref: '#/definitions/HeadPose'
      emotions:
        type: object
        properties:
          anger:
            $ref: '#/definitions/AzureEmotion'
          contempt:
            $ref: '#/definitions/AzureEmotion'
          disgust:
            $ref: '#/definitions/AzureEmotion'
          fear:
            $ref: '#/definitions/AzureEmotion'
          happiness:
            $ref: '#/definitions/AzureEmotion'
          neutral:
            $ref: '#/definitions/AzureEmotion'
          sadness:
            $ref: '#/definitions/AzureEmotion'
          surprise:
            $ref: '#/definitions/AzureEmotion'
      makeup:
        $ref: '#/definitions/Makeup'
      occlusion:
        $ref: '#/definitions/Occlusion'
      accessories:
        type: array
        items:
          $ref: '#/definitions/Accessory'
      noise:
        $ref: '#/definitions/Noise'
      blur:
        $ref: '#/definitions/AzureBlur'
      exposure:
        $ref: '#/definitions/AzureExposure'
  GCloudFace:
    type: object
    properties:
      faceRectangle:
        $ref: '#/definitions/BoundingBox'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/FaceLandmarksAnnotation'
      headPose:
        $ref: '#/definitions/HeadPose'
      emotions:
        type: object
        properties:
          anger:
            $ref: '#/definitions/GCloudEmotion'
          happiness:
            $ref: '#/definitions/GCloudEmotion'
          sadness:
            $ref: '#/definitions/GCloudEmotion'
          surprise:
            $ref: '#/definitions/GCloudEmotion'
      blur:
        type: object
        properties:
          blurredLikelihood:
            $ref: '#/definitions/ConfidenceLabel'
      exposure:
        type: object
        properties:
          underExposedLikelihood:
            $ref: '#/definitions/ConfidenceLabel'
  ImageAnnotation:
    type: object
    properties:
      metadata:
        $ref: '#/definitions/Metadata'
      texts:
        type: array
        items:
          $ref: '#/definitions/Text'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/Landmark'
      objects:
        type: array
        items:
          $ref: '#/definitions/Tag'
      tags:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      description:
        $ref: '#/definitions/Description'
      graphicalData:
        $ref: '#/definitions/GraphicalData'
      faces:
        type: array
        items:
          $ref: '#/definitions/Face'
      safetyAnnotations:
        $ref: '#/definitions/SafetyAnnotation'
  AzureImageAnnotation:
    type: object
    properties:
      metadata:
        $ref: '#/definitions/Metadata'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      objects:
        type: array
        items:
          $ref: '#/definitions/Tag'
      tags:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      description:
        $ref: '#/definitions/Description'
      graphicalData:
        $ref: '#/definitions/AzureGraphicalData'
      faces:
        type: array
        items:
          $ref: '#/definitions/AzureFace'
      safetyAnnotations:
        $ref: '#/definitions/AzureSafetyAnnotation'
  GCloudImageAnnotation:
    type: object
    properties:
      texts:
        type: array
        items:
          $ref: '#/definitions/Text'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/Landmark'
      objects:
        type: array
        items:
          $ref: '#/definitions/Tag'
      tags:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      description:
        $ref: '#/definitions/GCloudDescription'
      graphicalData:
        $ref: '#/definitions/GCloudGraphicalData'
      faces:
        type: array
        items:
          $ref: '#/definitions/GCloudFace'
      safetyAnnotations:
        $ref: '#/definitions/GCloudSafetyAnnotation'
