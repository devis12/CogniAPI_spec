swagger: '2.0'
schemes:
  - http # Remove if your API doesn't support HTTP
  - https # Remove if your API doesn't support HTTPS
host: cogni-api.herokuapp.com
basePath: /
info:
  # Describe your API here, you can use GFM (https://guides.github.com/features/mastering-markdown) here
  description: |
    CogniAPI aims to provide you a simpler and more straightforward way to analyse and process images content expoiting the power of Azure and Google Cloud cognitive services
    # Introduction
    This specification is intended to to be a good starting point for describing CogniAPI in 
    [OpenAPI/Swagger format](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md).
    
    # CogniAPI Specification
    The goal of The CogniAPI Specification is to guide you to the use of a less intricate but very useful REST service for image analysis which could be applied in many cases. Exploiting the power of Azure Computer Vision, Azure Face and Google Cloud Vision, this APIs will try to provide all the results to you in a unified and more straightforward schema. The properties provided to you could be resume in the following list:
    
      * Retrieving tags which tries to condensate the image content in a set of words
      * Detecting objects, locations and people of interest
      * Detecting faces (with emotions, gender, age and additional attributes related to them)
      * Evaluating the presence of inappropriate content in the image (racy, adult, violence, ...)
      * Complete analysis of the colors and other graphical aspects which characterize the image

  version: '1.0.0'

  title: CogniAPI

  termsOfService: 'https://cogni-api.herokuapp.com/terms/' 
  contact:
    email: devis.dalmoro@studenti.unitn.it 
  license:
    name: MIT
    url: 'https://github.com/devis12/CogniAPI/blob/master/LICENSE'
  x-logo:
    url: 'https://github.com/devis12/CogniAPI/blob/master/views/img/logo.png'
externalDocs:
  description: Find out how to create Github repo for your OpenAPI spec.
  url: 'https://github.com/Rebilly/generator-openapi-repo'
produces:
  # List of mime types CogniAPI endpoints can return.
  - application/json
  - text/plain
consumes:
  # List of mime types CogniAPI endpoints consumes.
  - application/json
  - application/x-www-form-urlencoded
  - application/form-data

# A list of tags used by the specification with additional metadata.
# The order of the tags can be used to reflect on their order by the parsing tools.
tags:
  - name: Analyse
    description: Image Analysis operations provided by CogniAPI
  - name: AnalyseBatch
    description: Image Batch Analysis operations provided by CogniAPI
  - name: GCloud
    description: Image Analysis operations provided just by Google Cloud Vision
  - name: Azure
    description: Image Analysis operations provided just by Azure Computer Vision and Azure Face
  - name: AzureFaceMg
    description: Face training operations provided just by Azure Face

paths:
          
  '/analyse': 
    get:
      tags:
        - Analyse
      
      summary: Complete image analysis 
      
      description: | 
        This call will probably be the most useful since it combines in a single response all the valuable information extracted from the cognitive services mentioned above (Google Cloud Vision, Azure Computer Vision and Azure Face) and showing them in a unified schema. If you are logged (specifying your username) you can also have the recognition services integrated in this answer: for every face you'll see the ones that are most similar to it, based on the previous faces you stored into the system with [/azure/faces/{user}](#/AzureFaceMg/AzureAddFace)
      
      operationId: analyse
      
      parameters:
      - in: query
        name: user
        type: string
        required: false
        description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags and objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            $ref: '#/definitions/ImageAnnotation'
        '400':
          description: Bad Request
          
  '/analyse/faces': 
    get:
      tags:
        - Analyse
      
      summary: Analysis of detected faces in the image
      
      description: | 
        This call will provide a full face detection and recognition of the image (the second one will be performed just if the user is logged)
      
      operationId: analyseFace
      
      parameters:
      - in: query
        name: user
        type: string
        required: false
        description: Optional user token which can be used to identify the logged user and to provide additional features (face recognition)
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              faces:
                type: array
                items:
                  $ref: '#/definitions/Face'
        '400':
          description: Bad Request
  
  '/analyse/tags': 
    get:
      tags:
        - Analyse
      
      summary: Image tag analysis 
      
      description: | 
        This call provides just tag and labels, respect to what it's possible to obtain with a full and complete [/analyse](#/Analyse/analyse) call
      
      operationId: analyseTag
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              tags:
                type: array
                items:
                  $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
          
  '/analyse/objects': 
    get:
      tags:
        - Analyse
      
      summary: Image objects detection 
      
      description: | 
        This call provides just detected objects, respect to what it's possible to obtain with a full and complete [/analyse](#/Analyse/analyse) call
      
      operationId: analyseObject
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              objects:
                type: array
                items:
                  $ref: '#/definitions/Tag'
        '400':
          description: Bad Request
          
  '/analyse/description': 
    get:
      tags:
        - Analyse
      
      summary: Description of image content
      
      description: | 
        This call provides a brief description of the image content: there will be a bunch of generic tags, grouped by concept, some captions and a classification based on a 86 category taxonomy defined here https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/category-taxonomy
      
      operationId: analyseDescription
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              description:
                $ref: '#/definitions/Description'
        '400':
          description: Bad Request
          
  '/analyse/texts': 
    get:
      tags:
        - Analyse
      
      summary: Text detection of supplied image
      
      description: | 
        This call will return the texts which have been detected inside the image
      
      operationId: analyseText
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              texts:
                type: array
                items:
                  $ref: '#/definitions/Text' 
        '400':
          description: Bad Request
  
  '/analyse/landmarks': 
    get:
      tags:
        - Analyse
      
      summary: Landmark detection of supplied image
      
      description: | 
        This call will return the landmarks (locations of interest) which have been detected inside the image
      
      operationId: analyseLandmark
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              landmarks:
                type: array
                items:
                  $ref: '#/definitions/Landmark'
        '400':
          description: Bad Request
          
  '/analyse/safety': 
    get:
      tags:
        - Analyse
      
      summary: Safety tags annotation of supplied image
      
      description: | 
        This call will return the safety properties (racy likelihood, level of violence detected in the image and so on) related to the linked image. This aims to evaluate the likelihood of inappropriate content in the image
      
      operationId: analyseSafety
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              safetyAnnotations:
                $ref: '#/definitions/SafetyAnnotation'
        '400':
          description: Bad Request
  
  '/analyse/colors': 
    get:
      tags:
        - Analyse
      
      summary: Color analysis of image content
      
      description: | 
        This call provides a complete analysis of the image colors 
      
      operationId: analyseColor
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              graphicalData:
                $ref: '#/definitions/GraphicalData'
        '400':
          description: Bad Request
          
  '/analyse/web': 
    get:
      tags:
        - Analyse
      
      summary: Web Detection analysis of image content
      
      description: | 
        This call provides an analysis of the image, supplying pages and other images founded in the web (fuly or partially) matching with the one analysed, in addition to a full web entities (related to the ones in Google Graph Network)
      
      operationId: analyseWebD
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              webDetection:
                type: array
                items:
                  $ref: '#/definitions/WebDetection'
        '400':
          description: Bad Request
          
  '/analyse/batch': 
    post:
      tags:
        - AnalyseBatch
      
      summary: Starting images batch analysis 
      
      description: | 
        This call will probably be the most useful since it combines in a single response all the valuable information extracted from the cognitive services mentioned above (Google Cloud Vision, Azure Computer Vision and Azure Face) and showing them in a unified schema. If you are logged (specifying your username) you can also have the recognition services integrated in this answer: for every face you'll see the ones that are most similar to it, based on the previous faces you stored into the system with [/azure/faces/{user}](#/AzureFaceMg/AzureAddFace). Respect to the simple [/analyse}](#/Analyse/analyse) call, this can process more images. It'll provide a token, which you could use in order to call [/analyse/batch/{token}/results](#/AnalyseBatch/analyseBatchResult) and get the final results when they're ready.
      
      operationId: analyseBatch
      
      parameters:
      - in: query
        name: user
        type: string
        required: false
        description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags and objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      - in: formData
        name: urls
        type: array
        items:
          type: string
        required: true
        description: Links of the images which have to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '202':
          description: Accepted
          schema: # response schema can be specified for each response
            type: object
            properties:
              btoken:
                type: string
                description: Token which you could use in order to call [/analyse/batch/{token}/results](#/AnalyseBatch/analyseBatchResult) and get the final results when they're ready.
        '400':
          description: Bad Request

  '/analyse/batch/{token}': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Complete images batch analysis results
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch)
      
      operationId: analyseBatchResults
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              $ref: '#/definitions/ImageAnnotation'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/faces': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis faces results
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the face objects
      
      operationId: analyseBatchFacesResults
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                faces: 
                  type: array
                  items:
                    $ref: '#/definitions/Face'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/tags': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis tag results
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the tag objects
      
      operationId: analyseBatchTags
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                tags:
                  type: array
                  items:
                    $ref: '#/definitions/TagNoBound'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/objects': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis detected objects
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the objects
      
      operationId: analyseBatchObjects
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                objects:
                  type: array
                  items:
                    $ref: '#/definitions/Tag'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/description': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis description results
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the description objects
      
      operationId: analyseBatchDescription
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                description: 
                  $ref: '#/definitions/Description'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/texts': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Complete images batch detected texts
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the text objects
      
      operationId: analyseBatchTexts
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                texts:
                  type: array
                  items:
                    $ref: '#/definitions/Text'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/landmarks': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch detected landmarks 
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the landmarks objects
      
      operationId: analyseBatchLandmarks
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                landmarks:
                  type: array
                  items:
                    $ref: '#/definitions/Landmark'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/safety': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis safety annotations
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the safety annotation objects
      
      operationId: analyseBatchSafety
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                safetyAnnotations:
                  $ref: '#/definitions/SafetyAnnotation'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/analyse/batch/{token}/colors': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis graphical data
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the graphical annotation objects
      
      operationId: analyseBatchColor
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                graphicalData:
                  $ref: '#/definitions/GraphicalData'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found

  '/analyse/batch/{token}/web': 
    get:
      tags:
        - AnalyseBatch
      
      summary: Images batch analysis web detection data
      
      description: | 
        Get all the image annotations which you requested with [/analyse/batch](#AnalyseBatch/analyseBatch) filtering out just the graphical annotation objects
      
      operationId: analyseBatchColor
      
      parameters:
      - in: path
        name: token
        type: string
        required: true
        description: Token which will be used to identify the processed request
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Ok
          schema: # response schema can be specified for each response
            type: array
            items:
              type: object
              properties:
                imageUrl:
                  type: string
                responseStatus:
                  $ref: '#/definitions/ResponseStatus'
                graphicalData:
                  $ref: '#/definitions/WebDetection'
        '204':
          description: No Content (Analysis is not completed yet)
        '400':
          description: Bad Request
        '404':
          description: Not Found
  
  '/gcloud/analyse': 
    get:
      tags:
        - GCloud
      
      summary: Complete image analysis 
      
      description: | 
        This call will provide in a single response all the valuable information extracted from the cognitive services using just Google Cloud Vision and showing them in a unified schema (no need to authenticate yourself because Google Cloud Vision doesn't supply any face recognition service)
      
      operationId: gcloudAnalyse
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags and objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            $ref: '#/definitions/GCloudImageAnnotation'
        '400':
          description: Bad Request
          
  '/gcloud/faces': 
    get:
      tags:
        - GCloud
      
      summary: Face detected in the image
      
      description: | 
        This call will provide a full face detection of the image with the data which Google Cloud Vision provides (no need to authenticate yourself because Google Cloud Vision doesn't supply any face recognition service)
      
      operationId: gcloudAnalyseFace
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              faces:
                type: array
                items:
                  $ref: '#/definitions/GCloudFace'
        '400':
          description: Bad Request
          
  '/gcloud/tags': 
    get:
      tags:
        - GCloud
      
      summary: Image tag analysis
      
      description: | 
        This call provides just tag and labels (returned by Google Cloud Vision), respect to what it's possible to obtain with a full and complete [/gcloud/analyse](#/GCloud/gcloudAnalyse) call
      
      operationId: gcloudAnalyseTag
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
        
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              tags:
                type: array
                items:
                  $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
          
  '/gcloud/objects': 
    get:
      tags:
        - GCloud
      
      summary: Image objects detection
      
      description: | 
        This call provides just detected objects (provided by Google Cloud Vision), respect to what it's possible to obtain with a full and complete [/gcloud/analyse](#/GCloud/gcloudAnalyse) call
      
      operationId: gcloudAnalyseObject
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              objects:
                type: array
                items:
                  $ref: '#/definitions/Tag'
        '400':
          description: Bad Request
          
  '/gcloud/description': 
    get:
      tags:
        - GCloud
      
      summary: Description of image content 
      description: | 
        This call provides a brief description of the image content: there will be a bunch of generic tags (supplied by Google Cloud Vision), grouped by concept 
      
      operationId: gcloudAnalyseDescription
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              description:
                $ref: '#/definitions/GCloudDescription'
        '400':
          description: Bad Request
          
  '/gcloud/texts': 
    get:
      tags:
        - GCloud
      
      summary: Text detection of supplied image 
      
      description: | 
        This call will return the different texts which have been detected inside the image by Google Cloud Vision
      
      operationId: gcloudAnalyseText
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              texts:
                type: array
                items:
                  $ref: '#/definitions/Text'
        '400':
          description: Bad Request
          
  '/gcloud/landmarks': 
    get:
      tags:
        - GCloud
      
      summary: Landmark detection of supplied image 
      
      description: | 
        This call will return the landmarks (locations of interest) which have been detected inside the image by Google Cloud Vision
      
      operationId: gcloudAnalyseLandmark
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              landmarks:
                type: array
                items:
                  $ref: '#/definitions/Landmark'
        '400':
          description: Bad Request
          
  '/gcloud/safety': 
    get:
      tags:
        - GCloud
      
      summary: Safety tags annotation of supplied image provided just by google cloud vision
      
      description: | 
        This call will return some safey properties (racy likelihood, violence inside the image and so on) with values related to the linked image. This aims to evaluate the likelihood of inappropriate content in the image
      
      operationId: gcloudAnalyseSafety
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              safetyAnnotations:
                $ref: '#/definitions/GCloudSafetyAnnotation'
        '400':
          description: Bad Request
  
  '/gcloud/colors': 
    get:
      tags:
        - GCloud
      
      summary: Color analysis of image content 
      description: | 
        This call provides an analysis of the image colors using Google Cloud Vision data
      
      operationId: gcloudAnalyseColor
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              graphicalData:
                $ref: '#/definitions/GCloudGraphicalData'
        '400':
          description: Bad Request
          
  '/gcloud/web': 
    get:
      tags:
        - GCloud
      
      summary: Web Detection analysis of image content
      
      description: | 
        This call provides an analysis of the image, supplying pages and other images founded in the web (fuly or partially) matching with the one analysed, in addition to a full web entities (related to the ones in Google Graph Network)
      
      operationId: gcloudAnalyseWebD
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              webDetection:
                $ref: '#/definitions/WebDetection'
        '400':
          description: Bad Request

  '/azure/analyse': 
    get:
      tags:
        - Azure
      
      summary: Complete image analysis 
      
      description: | 
        This call will combine in a single response all the valuable information extracted from Azure Computer Vision and Azure Face apis showing them in a unified schema (in respect to the standard one)
      
      operationId: azureAnalyse
      
      parameters:
      - in: query
        name: user
        type: string
        required: false
        description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
        
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
        
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags and objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            $ref: '#/definitions/AzureImageAnnotation'
        '400':
          description: Bad Request
          
  '/azure/faces': 
    get:
      tags:
        - Azure
      
      summary: Face detected in the image with recognition
      
      description: | 
        This call will provide a full face detection of the image with the data which Azure Computer Vision and Azure Face provide
      
      operationId: azureAnalyseFace
      
      parameters:
      - in: query
        name: user
        type: string
        required: false
        description: Optional user token which can be used to identify the logged user and to provide additional features (use /auth in order to get one)
        
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              faces:
                type: array
                items:
                  $ref: '#/definitions/AzureFace'
        '400':
          description: Bad Request
  
  '/azure/tags': 
    get:
      tags:
        - Azure
      
      summary: Image tag analysis 
      
      description: | 
        This call provides just tag and labels (supplied by Azure Computer Vision), respect to what it's possible to obtain with a full and complete [/azure/analyse](#/Azure/azureAnalyse) call
      
      operationId: azureAnalyseTag
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
        
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out tags with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
        
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              tags:
                type: array
                items:
                  $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request

  '/azure/objects': 
    get:
      tags:
        - Azure
      
      summary: Image objects detection 
      
      description: | 
        This call provides just detected objects (supplied by Azure Computer Vision), respect to what it's possible to obtain with a full and complete [/azure/analyse](#/Azure/AzureAnalyse) call
      
      operationId: azureAnalyseObject
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
        
      - in: query
        name: minscore
        type: number
        required: false
        default: 0
        minimum: 0
        maximum: 1
        description: Optional score value. You can set the minimum threshold in order to filter out objects with low confidence values. (If you go off the range [0,1] this value will be set to default automatically)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              objects:
                type: array
                items:
                  $ref: '#/definitions/Tag'
        '400':
          description: Bad Request
          
  '/azure/description': 
    get:
      tags:
        - Azure
      
      summary: Description of image content
      
      description: | 
        This call provides a brief description of the image content: there will be a bunch of generic tags, grouped by concept, some captions and a classification based on a 86 category taxonomy defined here https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/category-taxonomy
      
      operationId: azureAnalyseDescription
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              eresponseStatus:
                $ref: '#/definitions/ResponseStatus'
              description:
                $ref: '#/definitions/Description'
        '400':
          description: Bad Request
  
  '/azure/landmarks': 
    get:
      tags:
        - Azure
      
      summary: Landmark detection of supplied image
      
      description: | 
        This call will return the landmarks (locations of interest) which have been detected inside the image by Azure Computer Vision
        
      operationId: azureAnalyseLandmark
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              landmarks:
                type: array
                items:
                  $ref: '#/definitions/TagNoBound'
        '400':
          description: Bad Request
          
  '/azure/safety': 
    get:
      tags:
        - Azure
      
      summary: Safety tags annotation of supplied image
      
      description: | 
        This call will return some safey properties (racy likelihood, violence inside the image and so on) with values related to the linked image (data are supplied by Azure Computer Vision). This aims to evaluate the likelihood of inappropriate content in the image
        
      operationId: azureAnalyseSafety
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              safetyAnnotations:
                $ref: '#/definitions/AzureSafetyAnnotation'
        '400':
          description: Bad Request
  
  '/azure/colors': 
    get:
      tags:
        - Azure
      
      summary: Color analysis of image content
      
      description: | 
        This call provides a complete analysis of the image colors with data supplied by Azure Computer Vision
      
      operationId: azureAnalyseColor
      
      parameters:
      - in: query
        name: url
        type: string
        required: true
        description: Link of the image which has to be analysed
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
          schema: # response schema can be specified for each response
            type: object
            properties:
              imageUrl:
                type: string
              responseStatus:
                $ref: '#/definitions/ResponseStatus'
              graphicalData:
                $ref: '#/definitions/AzureGraphicalData'
        '400':
          description: Bad Request       
          
  '/azure/faces/{user}':   
    post:
      tags:
        - AzureFaceMg
      
      summary: Add a persisted face to your face group 
      
      description: | 
        This call provides a way for you to store some user data related to a face, so they will pop out again when a face similar to the one you're storing will be detected (mandatory to perform face training [azure/faces/train/{user}](#/AzureFaceMg/azureTrainFace) in the between, so it's possible to achieve this final result)
      
      operationId: azureAddFace
      
      consumes:
        - application/json
        - application/x-www-form-urlencoded
      
      parameters:
      - in: path
        name: user
        type: string
        required: true
        description: User token which can be used to identify the logged user in order to perform operations on the face group related to him
        
      - in: formData
        name: imageUrl
        type: string
        required: true
        description: Url of the image in which the face has been detected
        
      - in: formData
        name: target
        type: string
        required: true
        description: left,top,width,height values passed in this very order as a string separated by comma values (example '232,435,190,195'). You can retrieve this value from precedent analysis and face detection, where the top value it's just the y of the {tl} or {tr} corner and the left value it's the x coordinate of the {bl} or {tl} corner (width and height are already given in the faceRectangle) 
        
      - in: formData
        name: userData
        type: string
        required: true
        description: User data you want to associate to this face (e.g. name and surname)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
        '400':
          description: Bad Request     
    
    patch:
      tags:
        - AzureFaceMg
      
      summary: Update user data related to a persisted face
      
      description: | 
        This call provides a way for you to update the stored user data related to a face, so they will pop out again when a face similar to the one you're storing will be detected (mandatory to perform face training [azure/faces/train/{user}](#/AzureFaceMg/azureTrainFace) in the between, so it's possible to achieve this final result)
      
      operationId: azurePatchFace
      
      consumes:
        - application/json
        - application/x-www-form-urlencoded
      
      parameters:
      - in: path
        name: user
        type: string
        required: true
        description: User token which can be used to identify the logged user in order to perform operations on the face group related to him
        
      - in: formData
        name: persistedFaceId
        type: string
        required: true
        description: Id of the persisted face (you can have it in the similarFace arrat resulted from previous analysis)
        
      - in: formData
        name: userData
        type: string
        required: true
        description: User data you want to associate to this face (e.g. name and surname)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
        '400':
          description: Bad Request     
          
    delete:
      tags:
        - AzureFaceMg
      
      summary: Delete persisted face
      
      description: | 
        This call provides a way for you to delete the stored user data related to a face, so they won't pop out again when a face similar to the one you're storing will be detected (mandatory to perform face training [azure/faces/train/{user}](#/AzureFaceMg/azureTrainFace) in the between, so it's possible to achieve this final result)
      
      operationId: azureDeleteFace
      
      consumes:
        - application/json
        - application/x-www-form-urlencoded
      
      parameters:
      - in: path
        name: user
        type: string
        required: true
        description: User token which can be used to identify the logged user in order to perform operations on the face group related to him
        
      - in: formData
        name: persistedFaceId
        type: string
        required: true
        description: Id of the persisted face (you can have it in the similarFace array resulted from a previous analysis)
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '200':
          description: Success
        '400':
          description: Bad Request     

  
  '/azure/faces/train/{user}': 
    post:
      tags:
        - AzureFaceMg
      
      summary: Train your face group (mandatory after adding, patching or deleting face operations)
      
      description: | 
        This call provides a way for you to **START** training your cognitive service, in order to have a good face recognition system. Note carefully: the training can take time if you've stored a lot of faces and until it's done you won't be able to effectively use the face recognition system (**you'll get empty array in similar faces**)
      
      operationId: azureTrainFace
      
      parameters:
      - in: path
        name: user
        type: string
        required: true
        description: User token which can be used to identify the logged user in order to perform operations on the face group related to him
      
      # overwriting default global produces
      produces:
        - application/json
      responses: # list of responses
        '202':
          description: Accepted - Training has been started
        '400':
          description: Bad Request      

# Definitions of the object properties which characterize the APIs
definitions:
  
  Point2d:
    type: object
    properties:
      x:
        description: X coordinate in px
        type: integer
      y:
        description: Y coordinate in px
        type: integer
  
  Point3d:
    type: object
    properties:
      x:
        description: X coordinate in px
        type: integer
      y:
        description: Y coordinate in px
        type: integer
      z:
        description: Z coordinate in px (depth in an image)
        type: integer
  
  BoundingBox:
    type: object
    properties:
      width:
        description: Width dimension in px of the rectangle box
        type: integer
      height:
        description: Height dimension in px of the rectangle box
        type: integer
      bl:
        $ref: '#/definitions/Point2d'
      br:
        $ref: '#/definitions/Point2d'
      tl:
        $ref: '#/definitions/Point2d'
      tr:
        $ref: '#/definitions/Point2d'
  
  Exposure:
    type: object
    properties:
      exposureLevel:
        description: Face exposure level
        type: string
        enum: ['GoodExposure', 'OverExposure', 'UnderExposure']
      value:
        description: Level of exposure expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
      underExposedLikelihood:
        description: Google Cloud Likelihood value in order to express the level of exposure
        type: string
        enum: [ 'UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY, POSSIBLE', 'LIKELY', 'VERY_LIKELY' ]
        
  AzureExposure:
    type: object
    properties:
      exposureLevel:
        description: Face exposure level
        type: string
        enum: ['GoodExposure', 'OverExposure', 'UnderExposure']
      value:
        description: Level of exposure expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
  
  Blur:
    type: object
    properties:
      blurLevel:
        description: Blur level
        type: string
        enum: ['Low', 'Medium', 'High']
      value:
        description: Level of blur expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
      blurredLikelihood:
        description: Google Cloud Likelihood value in order to express the level of blurriness
        type: string
        enum: [ 'UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY, POSSIBLE', 'LIKELY', 'VERY_LIKELY' ]
        
  AzureBlur:
    type: object
    properties:
      blurLevel:
        description: Blur level
        type: string
        enum: ['Low', 'Medium', 'High']
      value:
        description: Level of blur expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
        
  Noise:
    type: object
    properties:
      noiseLevel:
        description: Noise level of face pixels
        type: string
        enum: ['Low', 'Medium', 'High']
      value:
        description: Level of noise of face pixels expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
        
  Occlusion:
    type: object
    properties:
      eyeMakeup:
        description: True if the eye(s) is(are) detected with makeup on it(them)
        type: boolean
      lipMakeup:
        description: True if the lip(s) has(have) makeup on it(them)
        type: boolean
  
  Makeup:
    type: object
    properties:
      foreheadOccluded:
        description: True if the forehead is occluded
        type: boolean
      eyeOccluded:
        description: True if the eye(s) is(are) occluded
        type: boolean
      mouthOccluded:
        description: True if the mouth is occluded
        type: boolean
        
  Accessory:
    type: object
    properties:
      type:
        description: Denomination of the type of identified accessory
        type: string
        example: headwear
      confidence:
        description: Probability value between 0 and 1 in order to express the confidence of the presence of the identified object 
        type: number
        minimum: 0
        maximum: 1
  
  Emotion:
    type: object
    properties:
      confidence:
        description: Probability value between 0 and 1 in order to express the confidence of the presence of the identified object 
        type: number
        minimum: 0
        maximum: 1
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
        
  AzureEmotion:
    type: object
    properties:
      confidence:
        description: Probability value between 0 and 1 in order to express the confidence of the presence of the identified object 
        type: number
        minimum: 0
        maximum: 1
        
  GCloudEmotion:
    type: object
    properties:
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
        
  HeadPose:
    type: object
    properties:
      rollAngle:
        description: Roll angle, which indicates the amount of clockwise/anti-clockwise rotation of the face relative to the image vertical about the axis perpendicular to the face. Range [-180,180]
        type: number
        minimum: -180
        maximum: 180
      panAngle:
        description: Yaw angle, which indicates the leftward/rightward angle that the face is pointing relative to the vertical plane perpendicular to the image. Range [-180,180].
        type: number
        minimum: -180
        maximum: 180
      tiltAngle:
        description: Pitch angle, which indicates the upwards/downwards angle that the face is pointing relative to the image's horizontal plane. Range [-180,180].
        type: number
        minimum: -180
        maximum: 180
  
  Hair:
    type: object
    properties:
      invisible:
        description: Indicate if the hair are visible or not
        type: boolean
      bald:
        description: Indicate level of baldness as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
      hairColors:
        description: Possible colors of the detected hair for the face
        type: array
        items:
          $ref: '#/definitions/HairColor'
  
  HairColor:
    type: object
    properties:
      color:
        description: Possible color for the hair
        type: string
        example: Brown
      confidence:
        description: Confidence value for this hair color expressed as a value between 0 and 1
        type: number
        minimum: 0
        maximum: 1
        
  FacialHair:
    type: object
    properties:
      moustache:
        description: Indicate if it's been detected a moustache in the face
        type: boolean
      beard:
        description: Indicate if it's been detected a beard in the face
        type: boolean
      sideburns:
        description: Indicate if it's been detected a sideburns in the face
        type: boolean
  
  PersistedFace:
    type: object
    properties:
      persistedFaceId:
        description: Persisted Face Id on Azure in the Face List Group related to the logged user
        type: string
      userData:
        description: User Data linked to the persisted Face Id on Azure in the Face List Group related to the logged user (and provided by him/her in the first place)
        type: string
      confidence:
        description: Level of similarity of this persisted face in respect to the one which has been just detected
        type: number
        minimum: 0
        maximum: 1
  
  FaceLandmarksAnnotation:
    type: object
    properties:
      type:
        type: string
        enum: [ 'UNKNOWN_LANDMARK', 'LEFT_EYE', 'RIGHT_EYE', 'LEFT_OF_LEFT_EYEBROW', 'RIGHT_OF_LEFT_EYEBROW', 'LEFT_OF_RIGHT_EYEBROW', 
                'RIGHT_OF_RIGHT_EYEBROW', 'MIDPOINT_BETWEEN_EYES', 'NOSE_TIP', 'UPPER_LIP', 'LOWER_LIP', 'MOUTH_LEFT', 'MOUTH_RIGHT',
                'MOUTH_CENTER', 'NOSE_BOTTOM_RIGHT', 'NOSE_BOTTOM_LEFT', 'NOSE_BOTTOM_CENTER', 'LEFT_EYE_TOP_BOUNDARY', 'LEFT_EYE_RIGHT_CORNER',  
                'LEFT_EYE_BOTTOM_BOUNDARY', 'LEFT_EYE_LEFT_CORNER', 'RIGHT_EYE_TOP_BOUNDARY', 'RIGHT_EYE_RIGHT_CORNER',
                'RIGHT_EYE_BOTTOM_BOUNDARY', 'RIGHT_EYE_LEFT_CORNER', 'LEFT_EYEBROW_UPPER_MIDPOINT', 'RIGHT_EYEBROW_UPPER_MIDPOINT', 'LEFT_EAR_TRAGION',
                'RIGHT_EAR_TRAGION', LEFT_EYE_PUPIL, 'RIGHT_EYE_PUPIL', 'FOREHEAD_GLABELLA', 'CHIN_GNATHION', 'CHIN_LEFT_GONION', 'CHIN_RIGHT_GONION'
              ]
      position:
        $ref: '#/definitions/Point3d'
  
  TagNoBound:
    type: object
    properties:
      name:
        description: Concept, object, logo, celebrity name
        type: string
      mid:
        description: Opaque entity ID. Some IDs may be available in Google Knowledge Graph Search API.
        type: string
        x-nullable: true
      confidence:
        description: Level of confidence for the detected concept, logo, object, celebrity
        type: number
        minimum: 0
        maximum: 1
  
  Tag:
    type: object
    properties:
      name:
        description: Concept, object, logo, celebrity name
        type: string
      mid:
        description: Opaque entity ID. Some IDs may be available in Google Knowledge Graph Search API.
        type: string
        x-nullable: true
      confidence:
        description: Level of confidence for the detected concept, logo, object, celebrity
        type: number
        minimum: 0
        maximum: 1
      boundingBox:
        $ref: '#/definitions/BoundingBox'
  
  Text:
    type: object
    properties:
      content:
        description: Content for the detected text
        type: string
      confidence:
        description: Level of confidence in relation to the presence of the detected text
        type: number
        minimum: 0
        maximum: 1
      locale:
        description: The language code for the locale in which the entity textual description is expressed
        type: string
      boundingBox:
        $ref: '#/definitions/BoundingBox'
  
  Landmark:
    type: object
    properties:
      name:
        description: Name of the detected landmark
        type: string
      mid:
        description: Opaque entity ID. Some IDs may be available in Google Knowledge Graph Search API.
        type: string
        x-nullable: true
      confidence:
        description: Level of confidence in relation to the presence of the detected landmark
        type: number
        minimum: 0
        maximum: 1
      latitude:
        description: Latitude coordinate for the detected landmark 
        type: number
        minimum: -90
        maximum: 90
        x-nullable: true
      longitude:
        description: Longitude coordinate for the detected landmark 
        type: number
        minimum: -180
        maximum: 180
        x-nullable: true
      boundingBox:
        $ref: '#/definitions/BoundingBox'
        x-nullable: true
  
  Description:
    type: object
    properties:
      generic_tags:
        description: Array of generic tags related to the uploaded image (grouped in lists by common concepts)
        type: array
        items:
          type: array
          items:
            type: string
      captions:
        description: Brief sentences which aims to synthesize the content of the image
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      categories:
        description: Most of the time just a single word which aims to synthesize the content of the image. All the values are limited to an 86 taxonomy defined by Azure Computer Vision at https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/category-taxonomy
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
          
  GCloudDescription:
    type: object
    properties:
      generic_tags:
        description: Array of generic tags related to the uploaded image (grouped in lists by common concepts)
        type: array
        items:
          type: array
          items:
            type: string
  
  RGBA:
    type: object
    properties:
      r:
        description: Red value for rgba
        type: integer
        minimum: 0
        maximum: 255
      g:
        description: Green value for rgba
        type: integer
        minimum: 0
        maximum: 255
      b:
        description: Blue value for rgba
        type: integer
        minimum: 0
        maximum: 255
      a:
        description: Alpha value
        type: number
        minimum: 0
        maximum: 1
        
  ColorInfoRGBA:
    type: array
    items:
      type: object
      properties:
        pixelFraction:
          description: The fraction of pixels the color occupies in the image. Value in range [0, 1]
          type: number
          minimum: 0
          maximum: 1
        confidence:
          description: Image-specific score for this color. Value in range [0, 1].
          type: number
          minimum: 0
          maximum: 1
        color:
          $ref: '#/definitions/RGBA'
  
  GraphicalData:
    type: object
    properties:
      dominantColorForeground:
        description: The dominant color in the foreground
        type: string
      dominantColorBackground:
        description: The dominant color in the background
        type: string
      dominantColors:
        description: The dominant colors for the image
        type: array
        items: 
          type: string
      accentColor:
        description: Accent color detected for the current image
        type: string
      isBWImg:
        description: True if the image is in black and white
        type: boolean
      clipArtType:
        description: Integer value between 0 and 3 in order to define how much this image is similar to a clipart
        type: integer
        minimum: 0
        maximum: 3
      lineDrawingType:
        description: True if the image content is detected as drawn by a human
        type: boolean
      colorInfoRGBA:
        $ref: '#/definitions/ColorInfoRGBA'
        
  AzureGraphicalData:
    type: object
    properties:
      dominantColorForeground:
        description: The dominant color in the foreground
        type: string
      dominantColorBackground:
        description: The dominant color in the background
        type: string
      dominantColors:
        description: The dominant colors for the image
        type: array
        items: 
          type: string
      accentColor:
        description: Accent color detected for the current image
        type: string
      isBWImg:
        description: True if the image is in black and white
        type: boolean
      clipArtType:
        description: Integer value between 0 and 3 in order to define how much this image is similar to a clipart
        type: integer
        minimum: 0
        maximum: 3
      lineDrawingType:
        description: True if the image content is detected as drawn by a human
        type: boolean
        
  GCloudGraphicalData:
    type: object
    properties:
      colorInfoRGBA:
        $ref: '#/definitions/ColorInfoRGBA'
  
  ConfidenceLabel:
    type: string
    description: Confidence value expressed as the likelihood label offered by Google Cloud Vision
    enum: [ 'UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY, POSSIBLE', 'LIKELY', 'VERY_LIKELY' ]
          
  SafetyProperty:
    type: object
    properties:
      present:
        description: True if the content of the image confirm the presence of this property (property 'racy' and present=True, it means that the content has a highly probability of been racist)
        type: boolean
      confidence:
        description: Confidence value expressed as a value between 0 and 1 for this safety property
        type: number
        minimum: 0
        maximum: 1
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
        
  AzureSafetyProperty:
    type: object
    properties:
      present:
        description: True if the content of the image confirm the presence of this property (property 'racy' and present=True, it means that the content has a highly probability of been racist)
        type: boolean
      confidence:
        description: Confidence value expressed as a value between 0 and 1 for this safety property
        type: number
        minimum: 0
        maximum: 1
        
  GCloudSafetyProperty:
    type: object
    properties:
      confidenceLabel:
        $ref: '#/definitions/ConfidenceLabel'
  
  SafetyAnnotation:
    type: object
    properties:
      racy:
        $ref: '#/definitions/SafetyProperty'
      adult:
        $ref: '#/definitions/SafetyProperty'
      violence:
        $ref: '#/definitions/SafetyProperty'
      medical:
        $ref: '#/definitions/SafetyProperty'
      spoof:
        $ref: '#/definitions/SafetyProperty'
        
  AzureSafetyAnnotation:
    type: object
    properties:
      racy:
        $ref: '#/definitions/AzureSafetyProperty'
      adult:
        $ref: '#/definitions/AzureSafetyProperty'
        
  GCloudSafetyAnnotation:
    type: object
    properties:
      racy:
        $ref: '#/definitions/GCloudSafetyProperty'
      adult:
        $ref: '#/definitions/GCloudSafetyProperty'
      violence:
        $ref: '#/definitions/GCloudSafetyProperty'
      medical:
        $ref: '#/definitions/GCloudSafetyProperty'
      spoof:
        $ref: '#/definitions/GCloudSafetyProperty'
  
  Metadata:
    type: object
    properties:
      width:
        description: Pixel width for the analyzed picture
        type: integer
      height:
        description: Pixel height for the analyzed picture
        type: integer
      format:
        description: Format of the analyzed picture
        type: string
  
  Face:
    type: object
    properties:
      faceId:
        description: faceId randomly generated by Azure Face (will expire in 24 hours)
        type: string
      gender:
        description: Recognized gender 
        type: string
        enum: ['male', 'female']
      age:
        description: Recognized age value
        type: integer
      smile:
        description: Smile intensity, a number between [0,1]
        type: number
        minimum: 0
        maximum: 1
      glasses:
        description: Glasses type
        type: string
        enum: ['NoGlasses', 'ReadingGlasses', 'Sunglasses', 'SwimmingGoggles']
      faceRectangle:
        $ref: '#/definitions/BoundingBox'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/FaceLandmarksAnnotation'
      similarFaces:
        type: array
        items:
          $ref: '#/definitions/PersistedFace'
      celebrity:
        $ref: '#/definitions/TagNoBound'
      facialHair:
        $ref: '#/definitions/FacialHair'
      hair:
        $ref: '#/definitions/Hair'
      headPose:
        $ref: '#/definitions/HeadPose'
      emotions:
        type: object
        properties:
          anger:
            $ref: '#/definitions/Emotion'
          contempt:
            $ref: '#/definitions/Emotion'
          disgust:
            $ref: '#/definitions/Emotion'
          fear:
            $ref: '#/definitions/Emotion'
          happiness:
            $ref: '#/definitions/Emotion'
          neutral:
            $ref: '#/definitions/Emotion'
          sadness:
            $ref: '#/definitions/Emotion'
          surprise:
            $ref: '#/definitions/Emotion'
      makeup:
        $ref: '#/definitions/Makeup'
      occlusion:
        $ref: '#/definitions/Occlusion'
      accessories:
        type: array
        items:
          $ref: '#/definitions/Accessory'
      noise:
        $ref: '#/definitions/Noise'
      blur:
        $ref: '#/definitions/Blur'
      exposure:
        $ref: '#/definitions/Exposure'
        
  AzureFace:
    type: object
    properties:
      faceId:
        description: faceId randomly generated by Azure Face (will expire in 24 hours)
        type: string
      gender:
        description: Recognized gender 
        type: string
        enum: ['male', 'female']
      age:
        description: Recognized age value
        type: integer
      smile:
        description: Smile intensity, a number between [0,1]
        type: number
        minimum: 0
        maximum: 1
      glasses:
        description: Glasses type
        type: string
        enum: ['NoGlasses', 'ReadingGlasses', 'Sunglasses', 'SwimmingGoggles']
      faceRectangle:
        $ref: '#/definitions/BoundingBox'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/FaceLandmarksAnnotation'
      similarFaces:
        type: array
        items:
          $ref: '#/definitions/PersistedFace'
      celebrity:
        $ref: '#/definitions/TagNoBound'
      facialHair:
        $ref: '#/definitions/FacialHair'
      hair:
        $ref: '#/definitions/Hair'
      headPose:
        $ref: '#/definitions/HeadPose'
      emotions:
        type: object
        properties:
          anger:
            $ref: '#/definitions/AzureEmotion'
          contempt:
            $ref: '#/definitions/AzureEmotion'
          disgust:
            $ref: '#/definitions/AzureEmotion'
          fear:
            $ref: '#/definitions/AzureEmotion'
          happiness:
            $ref: '#/definitions/AzureEmotion'
          neutral:
            $ref: '#/definitions/AzureEmotion'
          sadness:
            $ref: '#/definitions/AzureEmotion'
          surprise:
            $ref: '#/definitions/AzureEmotion'
      makeup:
        $ref: '#/definitions/Makeup'
      occlusion:
        $ref: '#/definitions/Occlusion'
      accessories:
        type: array
        items:
          $ref: '#/definitions/Accessory'
      noise:
        $ref: '#/definitions/Noise'
      blur:
        $ref: '#/definitions/AzureBlur'
      exposure:
        $ref: '#/definitions/AzureExposure'
  
  GCloudFace:
    type: object
    properties:
      faceRectangle:
        $ref: '#/definitions/BoundingBox'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/FaceLandmarksAnnotation'
      headPose:
        $ref: '#/definitions/HeadPose'
      emotions:
        type: object
        properties:
          anger:
            $ref: '#/definitions/GCloudEmotion'
          happiness:
            $ref: '#/definitions/GCloudEmotion'
          sadness:
            $ref: '#/definitions/GCloudEmotion'
          surprise:
            $ref: '#/definitions/GCloudEmotion'
      blur:
        type: object
        properties:
          blurredLikelihood:
            $ref: '#/definitions/ConfidenceLabel'
      exposure:
        type: object
        properties:
          underExposedLikelihood:
            $ref: '#/definitions/ConfidenceLabel'
  
  WebEntity:
    type: object
    description: Entity deduced from similar images on the Internet.
    properties:
      entityId:
        description: Opaque entity ID.
        type: string
      score:
        description: Overall relevancy score for the entity. Not normalized and not comparable across different image queries.
        type: number
        minimum: 0
        maximum: 1
      description:
        description: Canonical description of the entity, in English.
        type: string
  
  WebLabel:
    type: object
    description: Label to provide extra metadata for the web detection.
    properties:
      label:
        description: Label for extra metadata.
        type: string
      languageCode:
        description: The BCP-47 language code for label, such as "en-US" or "sr-Latn". For more information, see http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        type: string

  WebImage:
    type: object
    description: Metadata for online images.
    properties:
      url:
        description: The result image URL.
        type: string
      score:
        description: (Deprecated) Overall relevancy score for the image.
        type: number
        minimum: 0
        maximum: 1

  WebPage:
    type: object
    description: Metadata for web pages.
    properties:
      url:
        description: The result web page URL.
        type: string
      score:
        description: (Deprecated) Overall relevancy score for the web page.
        type: number
        minimum: 0
        maximum: 1
      pageTitle:
        description: Title for the web page, may contain HTML markups.
        type: string
      fullMatchingImages:
        description: Fully matching images on the page. Can include resized copies of the query image.
        type: array
        items:
          $ref: '#/definitions/WebImage'
      partialMatchingImages:
        description: Partial matching images on the page. Those images are similar enough to share some key-point features. For example an original image will likely have partial matching for its crops.
        type: array
        items:
          $ref: '#/definitions/WebImage'

  WebDetection:
    type: object
    description: Relevant information for the image from the Internet.
    properties:
      webEntities:
        type: array
        items:
          $ref: '#/definitions/WebEntity'
      fullMatchingImages:
        type: array
        items:
          $ref: '#/definitions/WebImage'
      partialMatchingImages:
        type: array
        items:
          $ref: '#/definitions/WebImage'
      pagesWithMatchingImages:
        type: array
        items:
          $ref: '#/definitions/WebPage'
      visualSimilarImages:
        type: array
        items:
          $ref: '#/definitions/WebImage'
      bestGuessLabels:
        type: array
        items:
          $ref: '#/definitions/WebLabel'

  ResponseStatus:
    type: object
    description: Response status code, with a better explanation of what could be the possible causes of the error
    properties:
      status:
        type: number
        default: 200
      code:
        type: string
        default: OK
      msg:
        type: string
        description: Additional string attached to the status in order to understand better the nature of the error
        
  ImageAnnotation:
    type: object
    properties:
      imageUrl:
        type: string
      metadata:
        $ref: '#/definitions/Metadata'
      texts:
        type: array
        items:
          $ref: '#/definitions/Text'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/Landmark'
      objects:
        type: array
        items:
          $ref: '#/definitions/Tag'
      tags:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      description:
        $ref: '#/definitions/Description'
      graphicalData:
        $ref: '#/definitions/GraphicalData'
      faces:
        type: array
        items:
          $ref: '#/definitions/Face'
      safetyAnnotations:
        $ref: '#/definitions/SafetyAnnotation'
      webDetection:
        $ref: '#/definitions/WebDetection'
      responseStatus:
        $ref: '#/definitions/ResponseStatus'
        
  AzureImageAnnotation:
    type: object
    properties:
      imageUrl:
        type: string
      metadata:
        $ref: '#/definitions/Metadata'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      objects:
        type: array
        items:
          $ref: '#/definitions/Tag'
      tags:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      description:
        $ref: '#/definitions/Description'
      graphicalData:
        $ref: '#/definitions/AzureGraphicalData'
      faces:
        type: array
        items:
          $ref: '#/definitions/AzureFace'
      safetyAnnotations:
        $ref: '#/definitions/AzureSafetyAnnotation'
      responseStatus:
        $ref: '#/definitions/ResponseStatus'
        
  GCloudImageAnnotation:
    type: object
    properties:
      imageUrl:
        type: string
      metadata:
        $ref: '#/definitions/Metadata'
      texts:
        type: array
        items:
          $ref: '#/definitions/Text'
      landmarks:
        type: array
        items:
          $ref: '#/definitions/Landmark'
      objects:
        type: array
        items:
          $ref: '#/definitions/Tag'
      tags:
        type: array
        items:
          $ref: '#/definitions/TagNoBound'
      description:
        $ref: '#/definitions/GCloudDescription'
      graphicalData:
        $ref: '#/definitions/GCloudGraphicalData'
      faces:
        type: array
        items:
          $ref: '#/definitions/GCloudFace'
      safetyAnnotations:
        $ref: '#/definitions/GCloudSafetyAnnotation'
      webDetection:
        $ref: '#/definitions/WebDetection'
      responseStatus:
        $ref: '#/definitions/ResponseStatus'